---
layout: single
title: "LLM 관련 기초 용어 정리"
date: 2025-11-04 22:30:00 +0900
categories: ai
tags: [llm, ai]
excerpt: "tokenizer, context 등의 llm 관련 기초 용어 정리 내용입니다."
---

## LLM 관련 용어 정리

### Tokenizer

- 텍스트를 모델이 이해할 수 있는 숫자 단위로 변환
- Word, Character, Subword level Tokenizer가 존재

**Subword Tokenizer**
- tiktoken 등
- 자주 쓰이는 단어는 하나의 토큰으로 분류
- 드문 단어는 subword로 분해

### LLM 파라미터

**파라미터의 역할**
- 각 파라미터는 기본적으로 신경망의 가중치(weights)와 편향(biases)을 말함
- 주로 행렬 연산에서 사용되는 값들로, 입력 프롬프트를 처리하고 다음 토큰을 예측하는 데 필요한 정보를 담음

**파라미터의 변화**
- 학습 중에는 손실 함수와 역전파를 통해 최적값으로 조정되며 최적화된 파라미터를 가짐
- 추론 시에는 프리징됨

### 주요 용어

**hidden**: transformer에서 토큰을 표현하는 벡터의 차원

**tensor parallel**: hidden을 GPU 2개가 나누어 처리

**vocab**: 모델이 인식 가능한 토큰 단위

**sequence**:
- 하나의 완전한 대화/요청 단위 - 하나의 독립적인 요청-응답 쌍
- 예시:
```
# 이 전체가 하나의 시퀀스
사용자: "파이썬으로 Hello World를 출력하는 코드를 알려줘"
AI: "print('Hello World')"
```
- prompt + generated tokens 전체
- 배치 처리 = 여러 시퀀스를 동시에 처리 가능
- 각 사용자의 개별 요청이 하나의 시퀀스
- 8명이 동시에 질문하면 → 8개의 시퀀스가 생성
- 시퀀스마다 독립적인 KV 캐시 유지

**context**:
- 모델이 한 번에 처리할 수 있는 최대 텍스트 길이 (토큰 단위)
- 시퀀스 내에서 모델이 기억하고 있는 토큰 범위
- [이전 대화들] + [현재 사용자 입력] + [AI 응답 공간]
- 예시:
```
컨텍스트 윈도우 = 4096 토큰이라면
- 이전 대화: 2000 토큰
- 사용자 질문: 100 토큰
- AI 응답 여유 공간: 1996 토큰
```
- max-model-len: 이 컨텍스트 윈도우의 최대 크기 설정